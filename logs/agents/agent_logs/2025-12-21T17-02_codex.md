# Agent Log — 25GB RAM + Faster 40-Run Sweep

- Agent: Codex CLI
- Model: GPT-5.2
- Created (UTC): 2025-12-21T17:02:59+00:00

## Goal

Reduce RAM-related crashes (notably `A4_gap1_foldA`) and enable safe speedups under a ~25GB RAM budget.

## Changes

- `kaggle_clicks/time_agg.py`
  - Reduced per-entity state overhead further by storing hour blocks in compact `array('i'/'f')` buffers (instead of Python lists of Python objects).
- `kaggle_clicks/run_baseline_te.py`
  - Avoided an extra full DataFrame copy right before building the dense feature matrix (reduces peak RAM).

## Quick smoke runs

- `python -m kaggle_clicks.run_baseline_te --sample-parquet data/interim/train_sample_0p1pct.parquet --run-tag ram_smoke_postfix_default --n-estimators 10 --max-depth 4 --n-jobs 2`
- `python -m kaggle_clicks.run_baseline_te --sample-parquet data/interim/train_sample_0p1pct.parquet --run-tag ram_smoke_postfix_a4gap1 --n-estimators 10 --max-depth 4 --n-jobs 2 --time-agg-windows 1 2 4 8 16 24 48 96 168 --time-agg-gap-hours 1`

## Operational recommendation (25GB RAM)

For the 10% 20×2 sweep, start with:
- `--max-parallel-runs 2`
- `--n-jobs 1` (per run)

Then increase to `--max-parallel-runs 3` only if you observe stable memory (no swapping / no kills).

